{
  "dataset_revision": "0587c2b6499fbc68a7623439c2af2b24748968dc",
  "task_name": "SlovakParlaSentClassification",
  "mteb_version": "1.39.7",
  "scores": {
    "train": [
      {
        "accuracy": 0.527462,
        "f1": 0.505155,
        "f1_weighted": 0.533611,
        "scores_per_experiment": [
          {
            "accuracy": 0.472692,
            "f1": 0.452606,
            "f1_weighted": 0.480372
          },
          {
            "accuracy": 0.536923,
            "f1": 0.502721,
            "f1_weighted": 0.545456
          },
          {
            "accuracy": 0.504615,
            "f1": 0.48276,
            "f1_weighted": 0.509605
          },
          {
            "accuracy": 0.513846,
            "f1": 0.506176,
            "f1_weighted": 0.518303
          },
          {
            "accuracy": 0.598077,
            "f1": 0.563619,
            "f1_weighted": 0.597907
          },
          {
            "accuracy": 0.477692,
            "f1": 0.466565,
            "f1_weighted": 0.491956
          },
          {
            "accuracy": 0.455385,
            "f1": 0.448648,
            "f1_weighted": 0.458454
          },
          {
            "accuracy": 0.571923,
            "f1": 0.540609,
            "f1_weighted": 0.575358
          },
          {
            "accuracy": 0.590385,
            "f1": 0.562924,
            "f1_weighted": 0.600671
          },
          {
            "accuracy": 0.553077,
            "f1": 0.524919,
            "f1_weighted": 0.558027
          }
        ],
        "main_score": 0.527462,
        "hf_subset": "default",
        "languages": [
          "slk-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.532885,
        "f1": 0.493114,
        "f1_weighted": 0.535322,
        "scores_per_experiment": [
          {
            "accuracy": 0.557692,
            "f1": 0.518844,
            "f1_weighted": 0.557581
          },
          {
            "accuracy": 0.605769,
            "f1": 0.556257,
            "f1_weighted": 0.609452
          },
          {
            "accuracy": 0.469231,
            "f1": 0.441809,
            "f1_weighted": 0.482726
          },
          {
            "accuracy": 0.525,
            "f1": 0.505641,
            "f1_weighted": 0.541967
          },
          {
            "accuracy": 0.526923,
            "f1": 0.483958,
            "f1_weighted": 0.528791
          },
          {
            "accuracy": 0.503846,
            "f1": 0.482519,
            "f1_weighted": 0.513199
          },
          {
            "accuracy": 0.546154,
            "f1": 0.505201,
            "f1_weighted": 0.530795
          },
          {
            "accuracy": 0.548077,
            "f1": 0.477968,
            "f1_weighted": 0.537601
          },
          {
            "accuracy": 0.521154,
            "f1": 0.478871,
            "f1_weighted": 0.524612
          },
          {
            "accuracy": 0.525,
            "f1": 0.480075,
            "f1_weighted": 0.526491
          }
        ],
        "main_score": 0.532885,
        "hf_subset": "default",
        "languages": [
          "slk-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 111.30698895454407,
  "kg_co2_emissions": null
}